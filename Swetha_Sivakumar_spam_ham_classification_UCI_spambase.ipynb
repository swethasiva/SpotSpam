{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Swetha Sivakumar spam_ham_classification_UCI_spambase.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "zd8IrSb2JBOY"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIS_fk7jHn9l"
      },
      "source": [
        "# Spam ham detection on UCI Spambase dataset\n",
        "Author: Swetha Sivakumar \\\\\n",
        "USC ID: 5978959727\n",
        "\n",
        "In this notebook, I have tuned and trained 5 classification models which are as follows:\n",
        "\n",
        "\n",
        "*   Model 1 : Random Forest Classifier - an ensemble learning method\n",
        "*   Model 2 : XGBoost Classifier - an ensemble learning method\n",
        "*   Model 3 : Logistic Regresion \n",
        "*   Model 4 : Neural Network in Tensorflow and Keras\n",
        "*   Model 5 : Support Vector Machine\n",
        "\n",
        "Summary of results are as follows: \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYkC4RdtcsO_",
        "outputId": "acf8ecee-674e-4935-99e0-c1a44dbc26c5"
      },
      "source": [
        "t = PrettyTable(['Model Name', 'False Positive Rate', 'False Negative Rate', 'Overall Error Rate', 'Accuracy Score'])\n",
        "t.add_row(['Random Forest Classifier', np.mean(rf_false_positive_rates), np.mean(rf_false_negative_rates), np.mean(rf_overall_error_rates), np.mean(rf_accuracy_scores)])\n",
        "t.add_row(['XGBoost Classifier', np.mean(xgb_false_positive_rates), np.mean(xgb_false_negative_rates), np.mean(xgb_overall_error_rates), np.mean(xgb_accuracy_scores)])\n",
        "t.add_row(['Tensorflow Neural Network', np.mean(nn_false_positive_rates), np.mean(nn_false_negative_rates), np.mean(nn_overall_error_rates), np.mean(nn_accuracy_scores)])\n",
        "t.add_row(['Logistic Regression', np.mean(logistic_regression_false_positive_rates), np.mean(logistic_regression_false_negative_rates), np.mean(logistic_regression_overall_error_rates), np.mean(logistic_regression_accuracy_scores)])\n",
        "print(t)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------------------+----------------------+---------------------+----------------------+--------------------+\n",
            "|         Model Name        | False Positive Rate  | False Negative Rate |  Overall Error Rate  |   Accuracy Score   |\n",
            "+---------------------------+----------------------+---------------------+----------------------+--------------------+\n",
            "|  Random Forest Classifier | 0.02869575307495939  | 0.06786776759152449 | 0.04413043478260869  | 0.9558695652173913 |\n",
            "|     XGBoost Classifier    | 0.03300069621721977  |  0.0789083844332463 | 0.051086956521739134 | 0.9489130434782609 |\n",
            "| Tensorflow Neural Network | 0.025107655810835204 | 0.06732438831886348 |  0.0417391304347826  | 0.9830369560122492 |\n",
            "|    Logistic Regression    | 0.04806864186070499  | 0.10871531783133995 | 0.07195652173913045  | 0.9280434782608695 |\n",
            "+---------------------------+----------------------+---------------------+----------------------+--------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwjBYhvMc1Jz"
      },
      "source": [
        "# Conclusion: \n",
        "From the results summary, we can understand that the best models are Neural Network and Random Forest Classifier. But since, the neural network has been trained for 100 epochs and the data size is just 4600 emails, there are chances of over fitting. We can also see that despite the lesser accuracy of random forest, the overall error rate of Random Forest Classifier is very close to the Neural Network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5QHj37-wUvz"
      },
      "source": [
        "# Mounting Google Drive to colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyMhzVEMKYdy",
        "outputId": "3abdbad9-5e1c-4edb-b6cd-f373ecb9656a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utHLryNtwhWt"
      },
      "source": [
        "# Changing Current Working Directory to Spambase Datset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmhzScNwKpiI",
        "outputId": "2e8c54ee-5a93-4849-eec0-fd6c81dbba6f"
      },
      "source": [
        "cd drive/My Drive/spambase"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/spambase\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrjhCt31wovV"
      },
      "source": [
        "# Importing required Python libraries "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZMMpr3InR1c"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import svm\n",
        "import xgboost as xgb \n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential \n",
        "from keras.layers import InputLayer \n",
        "from keras.layers import Dense \n",
        "from keras.layers import Dropout\n",
        "from sklearn.metrics import confusion_matrix, classification_report, plot_confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from collections import Counter\n",
        "from sklearn.metrics import accuracy_score\n",
        "from prettytable import PrettyTable\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W53Ht0eUxIec"
      },
      "source": [
        "# Read the input .data file into a pandas dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "qhPHmkpOnUwF",
        "outputId": "e79bddaf-f75c-4263-838a-c18a51ed8ef5"
      },
      "source": [
        "df = pd.read_csv(\"spambase.data\")\n",
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>0.64</th>\n",
              "      <th>0.64.1</th>\n",
              "      <th>0.1</th>\n",
              "      <th>0.32</th>\n",
              "      <th>0.2</th>\n",
              "      <th>0.3</th>\n",
              "      <th>0.4</th>\n",
              "      <th>0.5</th>\n",
              "      <th>0.6</th>\n",
              "      <th>0.7</th>\n",
              "      <th>0.64.2</th>\n",
              "      <th>0.8</th>\n",
              "      <th>0.9</th>\n",
              "      <th>0.10</th>\n",
              "      <th>0.32.1</th>\n",
              "      <th>0.11</th>\n",
              "      <th>1.29</th>\n",
              "      <th>1.93</th>\n",
              "      <th>0.12</th>\n",
              "      <th>0.96</th>\n",
              "      <th>0.13</th>\n",
              "      <th>0.14</th>\n",
              "      <th>0.15</th>\n",
              "      <th>0.16</th>\n",
              "      <th>0.17</th>\n",
              "      <th>0.18</th>\n",
              "      <th>0.19</th>\n",
              "      <th>0.20</th>\n",
              "      <th>0.21</th>\n",
              "      <th>0.22</th>\n",
              "      <th>0.23</th>\n",
              "      <th>0.24</th>\n",
              "      <th>0.25</th>\n",
              "      <th>0.26</th>\n",
              "      <th>0.27</th>\n",
              "      <th>0.28</th>\n",
              "      <th>0.29</th>\n",
              "      <th>0.30</th>\n",
              "      <th>0.31</th>\n",
              "      <th>0.32.2</th>\n",
              "      <th>0.33</th>\n",
              "      <th>0.34</th>\n",
              "      <th>0.35</th>\n",
              "      <th>0.36</th>\n",
              "      <th>0.37</th>\n",
              "      <th>0.38</th>\n",
              "      <th>0.39</th>\n",
              "      <th>0.40</th>\n",
              "      <th>0.41</th>\n",
              "      <th>0.42</th>\n",
              "      <th>0.778</th>\n",
              "      <th>0.43</th>\n",
              "      <th>0.44</th>\n",
              "      <th>3.756</th>\n",
              "      <th>61</th>\n",
              "      <th>278</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.21</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.94</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.28</td>\n",
              "      <td>3.47</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.59</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.132</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.372</td>\n",
              "      <td>0.180</td>\n",
              "      <td>0.048</td>\n",
              "      <td>5.114</td>\n",
              "      <td>101</td>\n",
              "      <td>1028</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.06</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.23</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.75</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.06</td>\n",
              "      <td>1.03</td>\n",
              "      <td>1.36</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.16</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.276</td>\n",
              "      <td>0.184</td>\n",
              "      <td>0.010</td>\n",
              "      <td>9.821</td>\n",
              "      <td>485</td>\n",
              "      <td>2259</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.18</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.537</td>\n",
              "      <td>40</td>\n",
              "      <td>191</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.18</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.537</td>\n",
              "      <td>40</td>\n",
              "      <td>191</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.85</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.85</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.223</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.000</td>\n",
              "      <td>15</td>\n",
              "      <td>54</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      0  0.64  0.64.1  0.1  0.32   0.2  ...   0.43   0.44  3.756   61   278  1\n",
              "0  0.21  0.28    0.50  0.0  0.14  0.28  ...  0.180  0.048  5.114  101  1028  1\n",
              "1  0.06  0.00    0.71  0.0  1.23  0.19  ...  0.184  0.010  9.821  485  2259  1\n",
              "2  0.00  0.00    0.00  0.0  0.63  0.00  ...  0.000  0.000  3.537   40   191  1\n",
              "3  0.00  0.00    0.00  0.0  0.63  0.00  ...  0.000  0.000  3.537   40   191  1\n",
              "4  0.00  0.00    0.00  0.0  1.85  0.00  ...  0.000  0.000  3.000   15    54  1\n",
              "\n",
              "[5 rows x 58 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEInGnHjGI2q"
      },
      "source": [
        "# Checking for imbalanced data\n",
        "\n",
        "\n",
        "> We check if the number of data points in one class dominates significantly over the other class. Since, in cases of imbalanced data, the model trained on such a data will be biased towards the class with higher data-points. In this case, we can generate duplicate/new data points for the class with lesser number of data points inorder to bridge the gap.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gEnuHSLjrxb",
        "outputId": "94530610-fa68-4782-d830-68883a049191"
      },
      "source": [
        "Counter(df['1'])\n",
        "## From result below, We see that, we have 2788 non-spam mails and 1812 spam mails. Thus, we can conclude that the distribution is not very skewed towards any one class. "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({0: 2788, 1: 1812})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmiE49AcKgJR"
      },
      "source": [
        "# Create feature vector and target vector as dataframes "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFKj5cD2KZay"
      },
      "source": [
        "X = df.iloc[:, :-1]\n",
        "Y = df.iloc[:,-1:]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zd8IrSb2JBOY"
      },
      "source": [
        "# Feature Selection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MB3RCmG-HVx9"
      },
      "source": [
        "# [Feature Selection] Test 1: Checking for highly correlated features\n",
        "\n",
        "\n",
        "> Features with high correlation with another feature, then this feature might not help in improving the model. But these highly correlated features might be a cause of curse of dimensionality without adding any new knowledge to the model. Also, these correlated features 'might' cause a bias towards a certain class for classification tasks. Thus, it is better to remove every thing except one of the highly correlated features. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "is-YBsRmiXBY",
        "outputId": "e9d43e87-d23b-42bd-ccab-a773d97e1e66"
      },
      "source": [
        "## Generating correlation matrix of Spambase dataset \n",
        "correlation_matrix = df.corr().abs()\n",
        "print(correlation_matrix)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "               0      0.64    0.64.1  ...        61       278         1\n",
            "0       1.000000  0.016735  0.065684  ...  0.061387  0.089165  0.126323\n",
            "0.64    0.016735  1.000000  0.033579  ...  0.000268  0.022679  0.030318\n",
            "0.64.1  0.065684  0.033579  1.000000  ...  0.107462  0.070119  0.196840\n",
            "0.1     0.013270  0.006920  0.020240  ...  0.022081  0.021369  0.057394\n",
            "0.32    0.023120  0.023761  0.077737  ...  0.052290  0.002492  0.241958\n",
            "0.2     0.059650  0.024815  0.087624  ...  0.090177  0.082089  0.232741\n",
            "0.3     0.007647  0.003939  0.036725  ...  0.059680  0.008344  0.332255\n",
            "0.4     0.003970  0.016261  0.012044  ...  0.037578  0.040252  0.206915\n",
            "0.5     0.106241  0.003803  0.093843  ...  0.189252  0.248726  0.231680\n",
            "0.6     0.041171  0.032989  0.032135  ...  0.103314  0.087274  0.139088\n",
            "0.7     0.188441  0.006843  0.048304  ...  0.086795  0.115056  0.234651\n",
            "0.64.2  0.105811  0.040406  0.083197  ...  0.021773  0.020076  0.007711\n",
            "0.8     0.066416  0.018836  0.047644  ...  0.041965  0.105150  0.133034\n",
            "0.9     0.036768  0.009194  0.008580  ...  0.060994  0.169258  0.060085\n",
            "0.10    0.028425  0.005344  0.122150  ...  0.213995  0.151626  0.195987\n",
            "0.32.1  0.059393  0.009123  0.063896  ...  0.026527  0.003007  0.263236\n",
            "0.11    0.081906  0.018348  0.036314  ...  0.062676  0.064261  0.263338\n",
            "1.29    0.053504  0.033366  0.121665  ...  0.075136  0.046390  0.203777\n",
            "1.93    0.128256  0.055488  0.139314  ...  0.006529  0.007307  0.273657\n",
            "0.12    0.021282  0.015794  0.031139  ...  0.099465  0.075751  0.189839\n",
            "0.96    0.197061  0.018200  0.156641  ...  0.085320  0.051797  0.383265\n",
            "0.13    0.024358  0.008841  0.035665  ...  0.027776  0.103954  0.091907\n",
            "0.14    0.134053  0.020481  0.123724  ...  0.123040  0.165978  0.334924\n",
            "0.15    0.188143  0.001999  0.041181  ...  0.044872  0.080993  0.216206\n",
            "0.16    0.072530  0.043461  0.087879  ...  0.051204  0.043269  0.256680\n",
            "0.17    0.061710  0.038191  0.062417  ...  0.051804  0.059603  0.232928\n",
            "0.18    0.066443  0.030291  0.108857  ...  0.054398  0.096549  0.183374\n",
            "0.19    0.048698  0.029205  0.050615  ...  0.038770  0.067597  0.158765\n",
            "0.20    0.041264  0.021928  0.057703  ...  0.034731  0.056629  0.133501\n",
            "0.21    0.052817  0.027492  0.032514  ...  0.038999  0.064116  0.171063\n",
            "0.22    0.039079  0.018086  0.038905  ...  0.027448  0.045923  0.126890\n",
            "0.23    0.032069  0.003316  0.061852  ...  0.027730  0.046797  0.114195\n",
            "0.24    0.041028  0.024891  0.054735  ...  0.025917  0.006919  0.119904\n",
            "0.25    0.027701  0.004292  0.061687  ...  0.024531  0.044529  0.112734\n",
            "0.26    0.044969  0.024044  0.048307  ...  0.030234  0.045964  0.149197\n",
            "0.27    0.054692  0.028181  0.046469  ...  0.038098  0.045792  0.136093\n",
            "0.28    0.057338  0.023990  0.066970  ...  0.033201  0.003490  0.177990\n",
            "0.29    0.007965  0.008918  0.032418  ...  0.009486  0.013897  0.031024\n",
            "0.30    0.011148  0.019111  0.014782  ...  0.029227  0.049257  0.122803\n",
            "0.31    0.036110  0.014808  0.047040  ...  0.004833  0.028806  0.064762\n",
            "0.32.2  0.009712  0.015411  0.030939  ...  0.023657  0.026373  0.097359\n",
            "0.33    0.026083  0.025165  0.005784  ...  0.034583  0.056512  0.136592\n",
            "0.34    0.024308  0.002355  0.044296  ...  0.017277  0.036529  0.135632\n",
            "0.35    0.022126  0.019730  0.053448  ...  0.025917  0.040661  0.094576\n",
            "0.36    0.037128  0.016397  0.050621  ...  0.051855  0.095445  0.140353\n",
            "0.37    0.034071  0.023844  0.056628  ...  0.033363  0.046372  0.146110\n",
            "0.38    0.000958  0.009813  0.029351  ...  0.010153  0.005158  0.044667\n",
            "0.39    0.017763  0.015739  0.026328  ...  0.016893  0.010033  0.084004\n",
            "0.40    0.026517  0.007270  0.033190  ...  0.040831  0.055298  0.059597\n",
            "0.41    0.021235  0.049802  0.016417  ...  0.370978  0.112212  0.089551\n",
            "0.42    0.033313  0.018516  0.033098  ...  0.013992  0.006015  0.064678\n",
            "0.778   0.058342  0.014506  0.108054  ...  0.077389  0.036324  0.241771\n",
            "0.43    0.117398  0.009584  0.087671  ...  0.183149  0.201949  0.323769\n",
            "0.44    0.008852  0.001953  0.003320  ...  0.061659  0.042568  0.065105\n",
            "3.756   0.044488  0.002086  0.097410  ...  0.492639  0.162314  0.110030\n",
            "61      0.061387  0.000268  0.107462  ...  1.000000  0.475486  0.216121\n",
            "278     0.089165  0.022679  0.070119  ...  0.475486  1.000000  0.249208\n",
            "1       0.126323  0.030318  0.196840  ...  0.216121  0.249208  1.000000\n",
            "\n",
            "[58 rows x 58 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMMCZmCpJf_W"
      },
      "source": [
        "# Removing redundancy causing highly correlated features\n",
        "\n",
        "\n",
        "> Since correlation matrix, calculates correlatio between every possible pair of features, the matrix is symmetric. Thus, to save computational expense, we can process just the upper or lower triangle. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> For our case, we have considered the upper triangle of correlation matrix and weeded out a list of features with correlation greater than 0.95.  \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Li_QtQIai-Uo",
        "outputId": "d8bf89d3-a452-4940-fa12-b093a437b3d2"
      },
      "source": [
        "'''print(df.shape)\n",
        "upper = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(np.bool))\n",
        "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
        "print(to_drop)\n",
        "df.drop(to_drop, axis=1, inplace=True)\n",
        "print(df.shape)'''\n",
        "## We can notice that one feature, df['0.25'] has been found to be highly correlated.\n",
        "## Which means, almost all the variable of this dataset are not correlated and thus could 'potentially' be contributing to the target pattern\n",
        "## But after multiple runs of models with all features, features with correlated features removed and features set selected through Chi Square test, I have noticed that the performance improvement is not significant for any of the models in this specific case."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4600, 58)\n",
            "['0.25']\n",
            "(4600, 57)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwZnb3FVCW5l"
      },
      "source": [
        "#[Feature Selection] Test 2: Chi-Square test\n",
        "\n",
        "\n",
        "> A chi-square test is used to test the independence of two events. \n",
        "\n",
        "\n",
        "> When two features are independent,  It will have smaller Chi-Square value. Whereas, high Chi-Square value of a feature mens the feature is more dependent on the target and it can potentially impart additional knowledge to model training.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuKGWME6lpbT"
      },
      "source": [
        "## But after multiple runs of models with all features, features with correlated features removed and features set selected through Chi Square test, I have noticed that the performance improvement is not significant for any of the models in this specific case.\n",
        "'''chi2_selector = SelectKBest(chi2, k=10)\n",
        "X_kbest = chi2_selector.fit_transform(X, Y)\n",
        "print(df.shape)\n",
        "cols = chi2_selector.get_support(indices=True)\n",
        "X = df.iloc[:,cols]\n",
        "print(df.columns)\n",
        "print(features_df_new.columns)'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JBb75uuKwfD"
      },
      "source": [
        "# Splitting train and test\n",
        "\n",
        "> Usually, the data is split into train and test set. Model is trained on train set and tested on test set. But since in our case, we will be K-fold cross-validation on the entire dataset, the necessity for train_test_split is avoided.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiccTD5cKqNH"
      },
      "source": [
        "# x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEkCoOZDxfzl"
      },
      "source": [
        "# Convert Target variable into categorical variable\n",
        "This step is required since, sklearn models like SVM, RandomForestClassifier amongst others do not work on continous data, they expect their target variable to be categorical ie. 0/1 or 0 to n classes.\n",
        "\n",
        "In our case, we make target variable binary, ie. 0/1 where 1 stands for spam and 0 stands for ham. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMV2LLpwGhf5",
        "outputId": "bd80b831-ce4b-4f12-fc08-6ace4887f7f7"
      },
      "source": [
        "label_encoder = preprocessing.LabelEncoder()\n",
        "Y = label_encoder.fit_transform(Y)\n",
        "#y_test = label_encoder.fit_transform(y_test)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:251: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFuD7Sw2wpPa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5XvAOTQ3xlU"
      },
      "source": [
        "# Model 1: Random Forest Classifier "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcwMHrLv36nK"
      },
      "source": [
        "## Model performance with default hyperparameter (without Hyperparameter tuning) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRc0PIuBoG1s",
        "outputId": "380e431f-79da-4b1a-cef3-90c58c1640de"
      },
      "source": [
        "rf_clf = RandomForestClassifier(criterion=\"gini\", max_features=\"log2\", n_estimators=73, random_state = 42)\n",
        "rf_scores = cross_validate(rf_clf, X, Y, cv=10)\n",
        "np.mean(rf_scores['test_score'])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9395652173913044"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnVSM0lVxqFH"
      },
      "source": [
        "\n",
        "# What is Hyperparameter Tuning ?\n",
        "\n",
        "\n",
        "> Hyperparameters are features that cannot be learnt by the model during the training process. Instead, these are parameters that infact describe the model. Hyperparameters are usually selected by humans before starting the model training, and the selection is done using prior experience of domain experts or by 'Trial and error'\n",
        "\n",
        "\n",
        "### Hyperparameter tuning can be done in different ways. Two of the most prominent methods of hyperparameter tuning are GridSearch and Randomized Search. \n",
        "\n",
        "\n",
        "*   Grid Search \n",
        "*   Randomized Search\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Implementing Hyperparameter Tuning for RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BU5A_3itnJ6y"
      },
      "source": [
        "rf_clf = RandomForestClassifier()\n",
        "param_grid = [{'n_estimators': [x for x in range(50, 100)],\n",
        "                    'criterion': ['gini', 'entropy'],\n",
        "                    'max_features': ['sqrt', 'log2', None ]}]\n",
        "grid = RandomizedSearchCV(rf_clf, param_grid, n_jobs=-1, refit=True)\n",
        "grid.fit(X, Y)\n",
        "rf_clf = grid.best_estimator_       # rf_clf now contains the random_forest_classifier with the best performing hyperparameters for this task."
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74FA3Neko9e6"
      },
      "source": [
        "# Doing a 10-fold cross validation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwqgpsFNEll_"
      },
      "source": [
        "\n",
        "> We can use the in-built cross-validation methods like **cross_validate(), cross_validate_score() or cross_validate_predict()** from sklearn. \n",
        "\n",
        "> But since we need to print the confusion matrix values for each fold, we instead implement k-fold explicitly in the next cell.\n",
        "\n",
        "\n",
        "> To get the just the accuracy of every fold and not the confusion matrix, another sklearn method that can be used is  \n",
        "\n",
        "\n",
        "> We use **Stratified K-Fold cross validation** inorder to ensure that every split has equal distribution of the two classes of data. Stratified K-Fold distribution is an improved version of K-fold cross validation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfwBPooKXAGl"
      },
      "source": [
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeX2uy9HbyKU",
        "outputId": "c87eef41-6ce4-4e38-ae7e-4611119b8185"
      },
      "source": [
        "Y = pd.DataFrame(Y)\n",
        "X = pd.DataFrame(X)\n",
        "print(type(Y))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pv-zz_NXkM-C"
      },
      "source": [
        "### Utility Function used by all models to calculate F, FN, error rate and the result table\n",
        "def report_model_result(t, my_confusion_matrix, accuracy, accuracy_scores, false_positive_rates, false_negative_rates, overall_error_rates):\n",
        "    false_positives_count = (my_confusion_matrix[0][1]) / (my_confusion_matrix[0][1] + my_confusion_matrix[0][0])\n",
        "    false_negative_count = (my_confusion_matrix[1][0]) / (my_confusion_matrix[1][0]+my_confusion_matrix[1][1])\n",
        "    overall_error_rate = (my_confusion_matrix[1][0]+my_confusion_matrix[0][1]) / (my_confusion_matrix[0][0]+my_confusion_matrix[0][1] + my_confusion_matrix[1][0]+my_confusion_matrix[1][1])\n",
        "    accuracy_scores.append(accuracy)\n",
        "    false_positive_rates.append(false_positives_count)\n",
        "    false_negative_rates.append(false_negative_count)\n",
        "    overall_error_rates.append(overall_error_rate)\n",
        "    t.add_row([fold_counter, false_positives_count, false_negative_count, overall_error_rate, accuracy])\n",
        "    "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQ9h_cUWJeci",
        "outputId": "1264b184-fa4f-43f3-973b-0ed910d1f7e4"
      },
      "source": [
        "\n",
        "### Code to cross-validate using sklearn's inbuilt method\n",
        "'''rf_scores = cross_validate(rf_clf, X, Y, cv=10) \n",
        "np.mean(rf_scores['test_score'])''' \n",
        "### Code for implementing k-fold cross validation mannualy\n",
        "t = PrettyTable(['K-th fold number', 'False Positive Rate', 'False Negative Rate', 'Overall Error Rate', 'Accuracy Score'])\n",
        "rf_accuracy_scores = []\n",
        "rf_false_positive_rates = []\n",
        "rf_false_negative_rates = []\n",
        "rf_overall_error_rates = []\n",
        "fold_counter = 1\n",
        "for train_index, test_index in skf.split(X, Y): \n",
        "    x_train, x_test = X[X.index.isin(train_index)], X[X.index.isin(test_index)]\n",
        "    y_train, y_test = Y[Y.index.isin(train_index)], Y[Y.index.isin(test_index)] \n",
        "    rf_clf.fit(x_train, y_train.values.ravel()) \n",
        "    y_pred = rf_clf.predict(x_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    my_confusion_matrix = confusion_matrix(y_test, y_pred)\n",
        "    report_model_result(t, my_confusion_matrix, accuracy, rf_accuracy_scores, rf_false_positive_rates, rf_false_negative_rates, rf_overall_error_rates)\n",
        "    #confusion_matrix_display = ConfusionMatrixDisplay(confusion_matrix=my_confusion_matrix, display_labels=rf_clf.classes_).plot()\n",
        "    fold_counter += 1\n",
        "print(t)\n",
        "t = PrettyTable(['Avg. False Positive Rate', 'Avg. False Negative Rate', 'Avg. Overall Error Rate', 'Avg. Accuracy Score'])\n",
        "t.add_row([np.mean(rf_false_positive_rates), np.mean(rf_false_negative_rates), np.mean(rf_overall_error_rates), np.mean(rf_accuracy_scores)])\n",
        "print(t)\n",
        "### Description of False Positives and False negatives to understand the results \n",
        "### False Positive (FP) - Mail that is not spam, but is incorrectly being classified as spam. \n",
        "### False Negative (FN) - Mail that is spam, but is incorrectly seen as a non-spam email. "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------+----------------------+----------------------+----------------------+--------------------+\n",
            "| K-th fold number | False Positive Rate  | False Negative Rate  |  Overall Error Rate  |   Accuracy Score   |\n",
            "+------------------+----------------------+----------------------+----------------------+--------------------+\n",
            "|        1         | 0.039568345323741004 | 0.08791208791208792  | 0.058695652173913045 | 0.941304347826087  |\n",
            "|        2         | 0.02158273381294964  | 0.07142857142857142  | 0.041304347826086954 | 0.9586956521739131 |\n",
            "|        3         | 0.02867383512544803  | 0.049723756906077346 | 0.03695652173913044  | 0.9630434782608696 |\n",
            "|        4         | 0.025089605734767026 | 0.055248618784530384 | 0.03695652173913044  | 0.9630434782608696 |\n",
            "|        5         | 0.03942652329749104  | 0.055248618784530384 | 0.04565217391304348  | 0.9543478260869566 |\n",
            "|        6         | 0.02867383512544803  | 0.055248618784530384 |  0.0391304347826087  | 0.9608695652173913 |\n",
            "|        7         | 0.02867383512544803  | 0.06077348066298342  | 0.041304347826086954 | 0.9586956521739131 |\n",
            "|        8         | 0.02867383512544803  | 0.08287292817679558  |         0.05         |        0.95        |\n",
            "|        9         | 0.02867383512544803  | 0.08839779005524862  | 0.05217391304347826  | 0.9478260869565217 |\n",
            "|        10        | 0.017921146953405017 |  0.0718232044198895  |  0.0391304347826087  | 0.9608695652173913 |\n",
            "+------------------+----------------------+----------------------+----------------------+--------------------+\n",
            "+--------------------------+--------------------------+-------------------------+---------------------+\n",
            "| Avg. False Positive Rate | Avg. False Negative Rate | Avg. Overall Error Rate | Avg. Accuracy Score |\n",
            "+--------------------------+--------------------------+-------------------------+---------------------+\n",
            "|   0.02869575307495939    |   0.06786776759152449    |   0.04413043478260869   |  0.9558695652173913 |\n",
            "+--------------------------+--------------------------+-------------------------+---------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkl_m0eNlegU"
      },
      "source": [
        "**NOTE:** In the above result, we can see a significant increase in model performance through hyperparameter tuning. Hence, it is essential to choose out model parameters as fitting to our problem statement as possible.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YUA79HN4Vy-"
      },
      "source": [
        "# Model 2 : XGboost "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zilv-j2NZN5l"
      },
      "source": [
        "xgb_clf = xgb.XGBClassifier()\n",
        "param_grid = {\"learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n",
        " \"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n",
        " \"min_child_weight\" : [ 1, 3, 5, 7 ],\n",
        " \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n",
        " \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ] }\n",
        "grid = RandomizedSearchCV(xgb_clf, param_grid, n_jobs=-1, refit=True)\n",
        "grid.fit(X, Y.values.ravel())\n",
        "xgb_clf = grid.best_estimator_"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PmBILhvm7NI",
        "outputId": "6e01188e-2471-4e48-ef23-c40ce46f9729"
      },
      "source": [
        "### Code for implementing k-fold cross validation mannualy\n",
        "t = PrettyTable(['K-th fold number', 'False Positive Rate', 'False Negative Rate', 'Overall Error Rate', 'Accuracy Score'])\n",
        "xgb_accuracy_scores = []\n",
        "xgb_false_positive_rates = []\n",
        "xgb_false_negative_rates = []\n",
        "xgb_overall_error_rates = []\n",
        "fold_counter = 1\n",
        "for train_index, test_index in skf.split(X, Y): \n",
        "    x_train, x_test = X[X.index.isin(train_index)], X[X.index.isin(test_index)]\n",
        "    y_train, y_test = Y[Y.index.isin(train_index)], Y[Y.index.isin(test_index)] \n",
        "    xgb_clf.fit(x_train, y_train.values.ravel()) \n",
        "    y_pred = xgb_clf.predict(x_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    my_confusion_matrix = confusion_matrix(y_test, y_pred)\n",
        "    report_model_result(t, my_confusion_matrix, accuracy, xgb_accuracy_scores, xgb_false_positive_rates, xgb_false_negative_rates, xgb_overall_error_rates)\n",
        "    #confusion_matrix_display = ConfusionMatrixDisplay(confusion_matrix=my_confusion_matrix, display_labels=xgb_clf.classes_).plot()\n",
        "    fold_counter += 1\n",
        "print(t)\n",
        "t = PrettyTable(['Avg. False Positive Rate', 'Avg. False Negative Rate', 'Avg. Overall Error Rate', 'Avg. Accuracy Score'])\n",
        "t.add_row([np.mean(xgb_false_positive_rates), np.mean(xgb_false_negative_rates), np.mean(xgb_overall_error_rates), np.mean(xgb_accuracy_scores)])\n",
        "print(t)\n",
        "### Description of False Positives and False negatives to understand the results \n",
        "### False Positive (FP) - Mail that is not spam, but is incorrectly being classified as spam. \n",
        "### False Negative (FN) - Mail that is spam, but is incorrectly seen as a non-spam email. "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------+----------------------+----------------------+----------------------+--------------------+\n",
            "| K-th fold number | False Positive Rate  | False Negative Rate  |  Overall Error Rate  |   Accuracy Score   |\n",
            "+------------------+----------------------+----------------------+----------------------+--------------------+\n",
            "|        1         | 0.04316546762589928  | 0.08791208791208792  | 0.06086956521739131  | 0.9391304347826087 |\n",
            "|        2         | 0.02877697841726619  | 0.08791208791208792  | 0.05217391304347826  | 0.9478260869565217 |\n",
            "|        3         | 0.03225806451612903  |  0.0718232044198895  | 0.04782608695652174  | 0.9521739130434783 |\n",
            "|        4         | 0.02867383512544803  | 0.08839779005524862  | 0.05217391304347826  | 0.9478260869565217 |\n",
            "|        5         | 0.04659498207885305  | 0.055248618784530384 |         0.05         |        0.95        |\n",
            "|        6         | 0.02867383512544803  | 0.06629834254143646  | 0.043478260869565216 | 0.9565217391304348 |\n",
            "|        7         | 0.03942652329749104  |  0.0718232044198895  | 0.05217391304347826  | 0.9478260869565217 |\n",
            "|        8         | 0.021505376344086023 | 0.10497237569060773  | 0.05434782608695652  | 0.9456521739130435 |\n",
            "|        9         | 0.035842293906810034 | 0.08839779005524862  | 0.05652173913043478  | 0.9434782608695652 |\n",
            "|        10        | 0.025089605734767026 | 0.06629834254143646  | 0.041304347826086954 | 0.9586956521739131 |\n",
            "+------------------+----------------------+----------------------+----------------------+--------------------+\n",
            "+--------------------------+--------------------------+-------------------------+---------------------+\n",
            "| Avg. False Positive Rate | Avg. False Negative Rate | Avg. Overall Error Rate | Avg. Accuracy Score |\n",
            "+--------------------------+--------------------------+-------------------------+---------------------+\n",
            "|   0.03300069621721977    |    0.0789083844332463    |   0.051086956521739134  |  0.9489130434782609 |\n",
            "+--------------------------+--------------------------+-------------------------+---------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPgROBpE4hTb"
      },
      "source": [
        "# Model 3 : Neural Network using Tensorflow and Keras \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qu5QvR1DKZhS"
      },
      "source": [
        "nn_clf = Sequential()\n",
        "nn_clf.add(Dense(57, input_dim=57, activation='relu'))\n",
        "nn_clf.add(Dense(units = 128, activation='relu'))\n",
        "nn_clf.add(Dense(1, activation='sigmoid'))\n",
        "# Compile model\n",
        "nn_clf.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NgEBgs8nbdf",
        "outputId": "f11d415e-0a00-46bf-f296-07779245a48f"
      },
      "source": [
        "### Code for implementing k-fold cross validation mannualy\n",
        "t = PrettyTable(['K-th fold number', 'False Positive Rate', 'False Negative Rate', 'Overall Error Rate', 'Accuracy Score'])\n",
        "nn_accuracy_scores = []\n",
        "nn_false_positive_rates = []\n",
        "nn_false_negative_rates = []\n",
        "nn_overall_error_rates = []\n",
        "fold_counter = 1\n",
        "for train_index, test_index in skf.split(X, Y): \n",
        "    x_train, x_test = X[X.index.isin(train_index)], X[X.index.isin(test_index)]\n",
        "    y_train, y_test = Y[Y.index.isin(train_index)], Y[Y.index.isin(test_index)] \n",
        "    history = nn_clf.fit(x_train, y_train, batch_size=100, epochs=100, verbose = 0, validation_data=(x_test, y_test))\n",
        "    accuracy = np.mean(history.history['val_accuracy'])\n",
        "    ypred = nn_clf.predict(x_test)\n",
        "    mod_y_pred = []\n",
        "    for predicted in y_pred:\n",
        "      if predicted > 0.75:\n",
        "        mod_y_pred.append(1)\n",
        "      else:\n",
        "        mod_y_pred.append(0)\n",
        "    my_confusion_matrix = confusion_matrix(y_test, mod_y_pred)\n",
        "    report_model_result(t, my_confusion_matrix, accuracy, nn_accuracy_scores, nn_false_positive_rates, nn_false_negative_rates, nn_overall_error_rates)\n",
        "    #confusion_matrix_display = ConfusionMatrixDisplay(confusion_matrix=my_confusion_matrix, display_labels=xgb_clf.classes_).plot()\n",
        "    fold_counter += 1\n",
        "print(t)\n",
        "t = PrettyTable(['Avg. False Positive Rate', 'Avg. False Negative Rate', 'Avg. Overall Error Rate', 'Avg. Accuracy Score'])\n",
        "t.add_row([np.mean(nn_false_positive_rates), np.mean(nn_false_negative_rates), np.mean(nn_overall_error_rates), np.mean(nn_accuracy_scores)])\n",
        "print(t)\n",
        "### Description of False Positives and False negatives to understand the results \n",
        "### False Positive (FP) - Mail that is not spam, but is incorrectly being classified as spam. \n",
        "### False Negative (FN) - Mail that is spam, but is incorrectly seen as a non-spam email. "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------+----------------------+---------------------+----------------------+--------------------+\n",
            "| K-th fold number | False Positive Rate  | False Negative Rate |  Overall Error Rate  |   Accuracy Score   |\n",
            "+------------------+----------------------+---------------------+----------------------+--------------------+\n",
            "|        1         | 0.025179856115107913 | 0.07142857142857142 | 0.043478260869565216 | 0.9574999982118606 |\n",
            "|        2         | 0.025179856115107913 | 0.07142857142857142 | 0.043478260869565216 | 0.9679347771406174 |\n",
            "|        3         | 0.025089605734767026 | 0.06629834254143646 | 0.041304347826086954 | 0.9777826076745987 |\n",
            "|        4         | 0.025089605734767026 | 0.06629834254143646 | 0.041304347826086954 | 0.9846086972951889 |\n",
            "|        5         | 0.025089605734767026 | 0.06629834254143646 | 0.041304347826086954 | 0.9874782621860504 |\n",
            "|        6         | 0.025089605734767026 | 0.06629834254143646 | 0.041304347826086954 | 0.985956524014473  |\n",
            "|        7         | 0.025089605734767026 | 0.06629834254143646 | 0.041304347826086954 | 0.9892826074361801 |\n",
            "|        8         | 0.025089605734767026 | 0.06629834254143646 | 0.041304347826086954 | 0.9898043465614319 |\n",
            "|        9         | 0.025089605734767026 | 0.06629834254143646 | 0.041304347826086954 | 0.9926739048957824 |\n",
            "|        10        | 0.025089605734767026 | 0.06629834254143646 | 0.041304347826086954 | 0.9973478347063065 |\n",
            "+------------------+----------------------+---------------------+----------------------+--------------------+\n",
            "+--------------------------+--------------------------+-------------------------+---------------------+\n",
            "| Avg. False Positive Rate | Avg. False Negative Rate | Avg. Overall Error Rate | Avg. Accuracy Score |\n",
            "+--------------------------+--------------------------+-------------------------+---------------------+\n",
            "|   0.025107655810835204   |   0.06732438831886348    |    0.0417391304347826   |  0.9830369560122492 |\n",
            "+--------------------------+--------------------------+-------------------------+---------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99oZa28vKxzK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTHvW3uZ-URn"
      },
      "source": [
        "# Model 4 : Logistic Regression "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iu7kSArs7fUH"
      },
      "source": [
        "logistic_regression_clf = LogisticRegression()\n",
        "param_grid = {'solver': ['newton-cg', 'lbfgs', 'liblinear'],  \n",
        "              'penalty': ['l2'], \n",
        "              'C': [100, 10, 1.0, 0.1, 0.01], \n",
        "              'max_iter': [100, 1000, 10000, 5000]}\n",
        "grid = RandomizedSearchCV(logistic_regression_clf, param_grid, n_jobs=-1, refit=True)\n",
        "grid.fit(X, Y.values.ravel())\n",
        "logistic_regression_clf = grid.best_estimator_\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMKZfYa47AwQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "595a98e0-73a8-4e97-dcea-caf483c00ae8"
      },
      "source": [
        "### Code for implementing k-fold cross validation mannualy\n",
        "t = PrettyTable(['K-th fold number', 'False Positive Rate', 'False Negative Rate', 'Overall Error Rate', 'Accuracy Score'])\n",
        "logistic_regression_accuracy_scores = []\n",
        "logistic_regression_false_positive_rates = []\n",
        "logistic_regression_false_negative_rates = []\n",
        "logistic_regression_overall_error_rates = []\n",
        "fold_counter = 1\n",
        "for train_index, test_index in skf.split(X, Y): \n",
        "    x_train, x_test = X[X.index.isin(train_index)], X[X.index.isin(test_index)]\n",
        "    y_train, y_test = Y[Y.index.isin(train_index)], Y[Y.index.isin(test_index)] \n",
        "    logistic_regression_clf.fit(x_train, y_train.values.ravel()) \n",
        "    y_pred = logistic_regression_clf.predict(x_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    my_confusion_matrix = confusion_matrix(y_test, y_pred)\n",
        "    report_model_result(t, my_confusion_matrix, accuracy, logistic_regression_accuracy_scores, logistic_regression_false_positive_rates, logistic_regression_false_negative_rates, logistic_regression_overall_error_rates)\n",
        "    #confusion_matrix_display = ConfusionMatrixDisplay(confusion_matrix=my_confusion_matrix, display_labels=xgb_clf.classes_).plot()\n",
        "    fold_counter += 1\n",
        "print(t)\n",
        "t = PrettyTable(['Avg. False Positive Rate', 'Avg. False Negative Rate', 'Avg. Overall Error Rate', 'Avg. Accuracy Score'])\n",
        "t.add_row([np.mean(logistic_regression_false_positive_rates), np.mean(logistic_regression_false_negative_rates), np.mean(logistic_regression_overall_error_rates), np.mean(logistic_regression_accuracy_scores)])\n",
        "print(t)\n",
        "### Description of False Positives and False negatives to understand the results \n",
        "### False Positive (FP) - Mail that is not spam, but is incorrectly being classified as spam. \n",
        "### False Negative (FN) - Mail that is spam, but is incorrectly seen as a non-spam email. "
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------+----------------------+---------------------+---------------------+--------------------+\n",
            "| K-th fold number | False Positive Rate  | False Negative Rate |  Overall Error Rate |   Accuracy Score   |\n",
            "+------------------+----------------------+---------------------+---------------------+--------------------+\n",
            "|        1         | 0.06115107913669065  | 0.10989010989010989 | 0.08043478260869565 | 0.9195652173913044 |\n",
            "|        2         | 0.050359712230215826 | 0.11538461538461539 | 0.07608695652173914 | 0.9239130434782609 |\n",
            "|        3         | 0.043010752688172046 | 0.12154696132596685 | 0.07391304347826087 | 0.9260869565217391 |\n",
            "|        4         | 0.043010752688172046 | 0.11602209944751381 | 0.07173913043478261 | 0.9282608695652174 |\n",
            "|        5         | 0.07168458781362007  | 0.08287292817679558 | 0.07608695652173914 | 0.9239130434782609 |\n",
            "|        6         | 0.05017921146953405  | 0.07734806629834254 | 0.06086956521739131 | 0.9391304347826087 |\n",
            "|        7         | 0.043010752688172046 | 0.11602209944751381 | 0.07173913043478261 | 0.9282608695652174 |\n",
            "|        8         | 0.043010752688172046 | 0.16022099447513813 |  0.0891304347826087 | 0.9108695652173913 |\n",
            "|        9         | 0.017921146953405017 | 0.09944751381215469 |         0.05        |        0.95        |\n",
            "|        10        | 0.05734767025089606  | 0.08839779005524862 | 0.06956521739130435 | 0.9304347826086956 |\n",
            "+------------------+----------------------+---------------------+---------------------+--------------------+\n",
            "+--------------------------+--------------------------+-------------------------+---------------------+\n",
            "| Avg. False Positive Rate | Avg. False Negative Rate | Avg. Overall Error Rate | Avg. Accuracy Score |\n",
            "+--------------------------+--------------------------+-------------------------+---------------------+\n",
            "|   0.04806864186070499    |   0.10871531783133995    |   0.07195652173913045   |  0.9280434782608695 |\n",
            "+--------------------------+--------------------------+-------------------------+---------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pt5raLphcf_L"
      },
      "source": [
        "# Model 5 : Support Vector Machines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTYajIaccP9v"
      },
      "source": [
        "svm_clf = svm.SVC() \n",
        "param_grid = {'C': [0.1, 1, 10, 100, 1000],  \n",
        "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
        "              'kernel': ['linear']}\n",
        "grid = RandomizedSearchCV(svm_clf, param_grid, n_jobs=-1, refit=True)\n",
        "grid.fit(X, Y)\n",
        "svm_clf = grid.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nS7hwqb4cVMv"
      },
      "source": [
        "## \n",
        "### Code for implementing k-fold cross validation mannualy\n",
        "svm_clf = svm.SVC(kernel='linear') ## I have explicitly mentioned max_iter parameter, becuase for lesser values the lbfgs solver doesn't converge, which might lead to bd performance \n",
        "t = PrettyTable(['K-th fold number', 'False Positive Rate', 'False Negative Rate', 'Overall Error Rate', 'Accuracy Score'])\n",
        "svm_accuracy_scores = []\n",
        "svm_false_positive_rates = []\n",
        "svm_false_negative_rates = []\n",
        "svm_overall_error_rates = []\n",
        "fold_counter = 1\n",
        "for train_index, test_index in skf.split(X, Y): \n",
        "    x_train, x_test = X[X.index.isin(train_index)], X[X.index.isin(test_index)]\n",
        "    y_train, y_test = Y[Y.index.isin(train_index)], Y[Y.index.isin(test_index)] \n",
        "    svm_clf.fit(x_train, y_train.values.ravel()) \n",
        "    y_pred = svm_clf.predict(x_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    my_confusion_matrix = confusion_matrix(y_test, y_pred)\n",
        "    report_model_result(t, my_confusion_matrix, accuracy, svm_accuracy_scores, svm_false_positive_rates,svm_false_negative_rates, svm_overall_error_rates)\n",
        "    #confusion_matrix_display = ConfusionMatrixDisplay(confusion_matrix=my_confusion_matrix, display_labels=xgb_clf.classes_).plot()\n",
        "    fold_counter += 1\n",
        "print(t)\n",
        "t = PrettyTable(['Avg. False Positive Rate', 'Avg. False Negative Rate', 'Avg. Overall Error Rate', 'Avg. Accuracy Score'])\n",
        "t.add_row([np.mean(svm_false_positive_rates), np.mean(svm_false_negative_rates), np.mean(svm_overall_error_rates), np.mean(svm_accuracy_scores)])\n",
        "print(t)\n",
        "### Description of False Positives and False negatives to understand the results \n",
        "### False Positive (FP) - Mail that is not spam, but is incorrectly being classified as spam. \n",
        "### False Negative (FN) - Mail that is spam, but is incorrectly seen as a non-spam email. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ERQBdrvQFTd"
      },
      "source": [
        "**NOTE:** Both colab and Jupyter Notebook results in .ipynb notebok. But since colab uses GPUs provided by google, it is much faster than Jupyter Noteook. On the otherhand, Jupyter notebook uses the local CPU and my system isn't equipped with a good gpu, thus, I have used colab for faster processing. But I am capble of doing the same work on both colab and Jupyter Notebook. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6uiqj35D0ms"
      },
      "source": [
        "# Results Summary Accross all models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhCgNqsuDzuu",
        "outputId": "16d1f9e8-e96c-41cb-f5b3-55c73cd684b0"
      },
      "source": [
        "t = PrettyTable(['Model Name', 'False Positive Rate', 'False Negative Rate', 'Overall Error Rate', 'Accuracy Score'])\n",
        "t.add_row(['Random Forest Classifier', np.mean(rf_false_positive_rates), np.mean(rf_false_negative_rates), np.mean(rf_overall_error_rates), np.mean(rf_accuracy_scores)])\n",
        "t.add_row(['XGBoost Classifier', np.mean(xgb_false_positive_rates), np.mean(xgb_false_negative_rates), np.mean(xgb_overall_error_rates), np.mean(xgb_accuracy_scores)])\n",
        "t.add_row(['Tensorflow Neural Network', np.mean(nn_false_positive_rates), np.mean(nn_false_negative_rates), np.mean(nn_overall_error_rates), np.mean(nn_accuracy_scores)])\n",
        "t.add_row(['Logistic Regression', np.mean(logistic_regression_false_positive_rates), np.mean(logistic_regression_false_negative_rates), np.mean(logistic_regression_overall_error_rates), np.mean(logistic_regression_accuracy_scores)])\n",
        "print(t)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------------------+----------------------+---------------------+----------------------+--------------------+\n",
            "|         Model Name        | False Positive Rate  | False Negative Rate |  Overall Error Rate  |   Accuracy Score   |\n",
            "+---------------------------+----------------------+---------------------+----------------------+--------------------+\n",
            "|  Random Forest Classifier | 0.02869575307495939  | 0.06786776759152449 | 0.04413043478260869  | 0.9558695652173913 |\n",
            "|     XGBoost Classifier    | 0.03300069621721977  |  0.0789083844332463 | 0.051086956521739134 | 0.9489130434782609 |\n",
            "| Tensorflow Neural Network | 0.025107655810835204 | 0.06732438831886348 |  0.0417391304347826  | 0.9830369560122492 |\n",
            "|    Logistic Regression    | 0.04806864186070499  | 0.10871531783133995 | 0.07195652173913045  | 0.9280434782608695 |\n",
            "+---------------------------+----------------------+---------------------+----------------------+--------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwHxPzbgEIMW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}